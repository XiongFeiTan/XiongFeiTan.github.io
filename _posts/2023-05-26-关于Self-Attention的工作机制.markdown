---
layout: post
title: å…³äºSelf-Attentionå·¥ä½œæœºåˆ¶
categories: [æ·±åº¦å­¦ä¹ ]
comments: true
description: Attentionæ˜¯LLMsçš„æ ¸å¿ƒ,å¦‚ä½•ç®€å•ç†è§£ï¼Ÿ
---


# Attentionå·¥ä½œæœºåˆ¶
## 1. å‰ç½®
ğŸ’¡ æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰æ˜¯ä¸€ç§åœ¨æ·±åº¦å­¦ä¹ ä¸­è¢«å¹¿æ³›åº”ç”¨çš„æŠ€æœ¯ï¼Œå®ƒæ¨¡æ‹Ÿäº†äººç±»çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥åœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶æ›´åŠ å…³æ³¨é‡è¦çš„éƒ¨åˆ†ã€‚

## 2. Attention
### 2.1 Embeddingsç†è§£
NLPä¸­ï¼Œå°†Words Sequencesè½¬æ¢ä¸ºTokenï¼Œç„¶åå°†Tokenè½¬æ¢ä¸ºEmbeddings.
#### åµŒå…¥å‘é‡é€šå¸¸å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

**ç»´åº¦å›ºå®š**: åœ¨ä¸€ä¸ªç»™å®šçš„åµŒå…¥ç©ºé—´ä¸­ï¼Œæ¯ä¸ªç¬¦å·éƒ½ä¼šè¢«æ˜ å°„ä¸ºå›ºå®šç»´åº¦çš„å‘é‡ï¼Œè¿™ä¸ªç»´åº¦å¯ä»¥æ˜¯é¢„å…ˆè®¾å®šå¥½çš„ï¼Œä¹Ÿå¯ä»¥é€šè¿‡è®­ç»ƒæ•°æ®è‡ªåŠ¨å­¦ä¹ å¾—åˆ°ã€‚

**è¯­ä¹‰ç›¸å…³**: ç›¸ä¼¼çš„ç¬¦å·åœ¨åµŒå…¥ç©ºé—´ä¸­çš„å‘é‡è¡¨ç¤ºä¹Ÿä¼šæ›´åŠ æ¥è¿‘ï¼Œå³ç›¸ä¼¼çš„å•è¯åœ¨åµŒå…¥ç©ºé—´ä¸­çš„è·ç¦»è¾ƒè¿‘ã€‚

**ä¸Šä¸‹æ–‡æ„ŸçŸ¥**: åµŒå…¥å‘é‡çš„å–å€¼ä¼šå—åˆ°ç¬¦å·æ‰€å¤„çš„ä¸Šä¸‹æ–‡ç¯å¢ƒçš„å½±å“ï¼Œä»è€Œèƒ½å¤Ÿè¡¨è¾¾å‡ºä¸åŒè¯­å¢ƒä¸‹çš„å«ä¹‰å·®å¼‚ã€‚

#### åµŒå…¥å‘é‡çš„ä½œç”¨ï¼š
**é™ç»´**: å°†é«˜ç»´çš„ç¦»æ•£ç¬¦å·è¡¨ç¤ºæ˜ å°„åˆ°ä½ç»´çš„è¿ç»­å‘é‡ç©ºé—´ä¸­ï¼Œé™ä½äº†ç‰¹å¾è¡¨ç¤ºçš„ç»´åº¦ã€‚

**è¯­ä¹‰ä¿¡æ¯**: é€šè¿‡åµŒå…¥å‘é‡ï¼Œå¯ä»¥æ•æ‰åˆ°å•è¯ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ–‡æœ¬ã€‚

**ç‰¹å¾è¡¨è¾¾**: åµŒå…¥å‘é‡å¯ä»¥ä½œä¸ºå…¶ä»–æ¨¡å‹ï¼ˆå¦‚åˆ†ç±»å™¨ã€å›å½’å™¨ç­‰ï¼‰çš„è¾“å…¥ç‰¹å¾ï¼Œä»è€Œå¸®åŠ©æ¨¡å‹å­¦ä¹ æ›´å¥½çš„è¡¨ç¤ºå’Œè¿›è¡Œé¢„æµ‹ã€‚

åœ¨å®è·µä¸­ï¼ŒåµŒå…¥å‘é‡é€šå¸¸æ˜¯é€šè¿‡è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹æ¥å­¦ä¹ å¾—åˆ°çš„ã€‚é€šè¿‡åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å•è¯æˆ–å­—ç¬¦çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºå›ºå®šç»´åº¦çš„åµŒå…¥å‘é‡ã€‚

**æ€»ä¹‹** Embeddingæ˜¯ä¸€ç§å°†ç¦»æ•£ç¬¦å·æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´çš„æŠ€æœ¯ï¼Œé€šè¿‡åµŒå…¥å‘é‡ï¼Œå¯ä»¥å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºå¯ä»¥è¢«æ·±åº¦å­¦ä¹ æ¨¡å‹å¤„ç†çš„å½¢å¼ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ•æ‰åˆ°å•è¯æˆ–å­—ç¬¦ä¹‹é—´çš„è¯­ä¹‰å…³ç³»å’Œé‡è¦æ€§ã€‚


---
å› æ­¤Embeddingå¯ä»¥çœ‹åšæ˜¯é‡‡ç”¨ä¸€å †æ•°å­—æ¥å¯¹æ¯ä¸ªTokençš„æœ‰æ„ä¹‰çš„è¡¨ç¤º. å¦‚å›¾ ![avatar](/images/2023/attention-1.png)

<br>
è€Œå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œæ˜¯å¯¹äººç±»å±‚æ¬¡çš„ç†è§£ï¼Œåˆ†åˆ«ç‹¬ç«‹çš„å¤„ç†tokensæ˜¯éš¾ä»¥åšåˆ°è¯­ä¹‰ç†è§£çš„ï¼Œè¿˜éœ€è¦ç†è§£tokensä¹‹é—´çš„å…³ç³».å¦‚å›¾ 

![avatar](/images/2023/attention-2.png)

<br>
åœ¨ Self-Attention ä¸­ï¼Œtokens ä¹‹é—´çš„å…³ç³»è¡¨ç¤ºä¸ºæ¦‚ç‡åˆ†æ•°. æ¯ä¸ª token åˆ†é…æœ€é«˜çš„åˆ†æ•°ç»™å…¶è‡ªå·±ï¼ŒåŸºäºç›¸å…³æ€§ç»™äºå…¶ä»– tokens åˆ†æ•°.

![avatar](/images/2023/attention-3.png)

ä¸ºäº†ç†è§£ self-attention æ˜¯å¦‚ä½•è¿›è¡Œçš„ï¼Œé¦–å…ˆéœ€è¦ç†è§£ä¸‰ä¸ªæ¦‚å¿µï¼š
Query Vector
Key Vector
Value Vector
è¿™äº›å‘é‡æ˜¯é€šè¿‡å¯¹ Input Embedding ä¹˜ä»¥ä¸‰ä¸ªå¯è®­ç»ƒçš„æƒé‡çŸ©é˜µæ¥å¾—åˆ°çš„.
![avatar](/images/2023/attention-4.png)

<br>

**Self-attention** å¯ä»¥è®©æ¨¡å‹å­¦ä¹ åˆ°åºåˆ—ä¸­ä¸åŒéƒ¨åˆ†ä¹‹é—´é•¿èŒƒå›´çš„ä¾èµ–(Long Range Dependencies).å½“å¾—åˆ° Keys, Querieså’ŒValues ä¹‹åï¼Œå°†å…¶åˆå¹¶ä¸€èµ·ï¼Œä»¥åˆ›å»ºæ–°çš„Context-Aware Embeddingsé›†åˆ.
![avatar](/images/2023/attention-5.png)


<br>
PyTorch ä¸­ Self-Attention çš„å®ç°ç¤ºä¾‹å¦‚:

![avatar](/images/2023/attention-6.png)

<br>
åœ¨PyTorchä¸­ï¼Œè¿˜å¯ä»¥ä½¿ç”¨torch.nn.MultiheadAttentionæ¨¡å—æ¥å®ç°Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰æœºåˆ¶, ä¸€ä¸‹æ˜¯ç®€å•çš„demoæ¼”ç¤ºï¼š

```
import torch
import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, embed_size, num_heads):
        super(SelfAttention, self).__init__()
        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads)
        self.linear = nn.Linear(embed_size, embed_size)
        self.layer_norm = nn.LayerNorm(embed_size)
    
    def forward(self, inputs):
        # è®¡ç®—Self-Attention
        attended_output, _ = self.attention(inputs, inputs, inputs)
        
        # æ®‹å·®è¿æ¥å’ŒLayer Normalization
        residual = inputs + attended_output
        normalized_output = self.layer_norm(residual)
        
        # å‰é¦ˆç¥ç»ç½‘ç»œ
        output = self.linear(normalized_output)
        
        return output

```

ä¸Šè¿°ä»£ç ä¸­ï¼Œå®šä¹‰äº†ä¸€ä¸ªåä¸ºSelfAttentionçš„è‡ªæ³¨æ„åŠ›æ¨¡å‹ã€‚

**é¦–å…ˆ** åœ¨æ¨¡å‹çš„åˆå§‹åŒ–å‡½æ•°ä¸­ï¼Œä½¿ç”¨nn.MultiheadAttentionå®šä¹‰äº†ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ï¼Œå…¶ä¸­embed_sizeè¡¨ç¤ºè¾“å…¥çš„åµŒå…¥ç»´åº¦ï¼Œnum_headsè¡¨ç¤ºæ³¨æ„åŠ›å¤´æ•°ã€‚åœ¨å‰å‘ä¼ æ’­å‡½æ•°ä¸­ï¼Œé€šè¿‡è°ƒç”¨self.attentionå¯¹è¾“å…¥è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—ï¼Œå¾—åˆ°attended_outputã€‚
<br>
**æ¥ç€** å°†è¾“å…¥ä¸attended_outputè¿›è¡Œæ®‹å·®è¿æ¥ï¼ˆelement-wiseç›¸åŠ ï¼‰ï¼Œç„¶åä½¿ç”¨nn.LayerNormè¿›è¡Œå±‚å½’ä¸€åŒ–æ“ä½œã€‚
<br>
**æœ€å** å°†å½’ä¸€åŒ–åçš„è¾“å‡ºé€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚self.linearè¿›è¡Œæ˜ å°„ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºç»“æœã€‚
<br>
å½“ç„¶åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿˜éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å’Œæ¨¡å‹ç»“æ„è¿›è¡Œé€‚å½“çš„è°ƒæ•´å’Œæ‰©å±•ã€‚æ­¤å¤„ä»…æä¾›äº†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œä¾›ç®€å•ç†è§£å’Œå­¦ä¹ ã€‚

---
<br>

### 5. å‚è€ƒæ–‡æ¡£
**å›¾ç‰‡è½¬è½½å¼•ç”¨twitter**:
https://twitter.com/akshay_pachaar/status/1657368551471333384


