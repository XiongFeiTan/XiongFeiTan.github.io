---
layout: post
title: Mathematical for machine learning-(1)
categories: [Math]
comments: true
description: ML数学梳理.
---

## 前言：

![Hello Math](.\ai.png)

对于从事IT工程方面的同学来说，刚开始绝大部分同学可能和我有一样的想法，就是对于一个算法模型，能够根据给定的输入来获得输出，相当于当成一个黑盒调用，不会太关注这个深层的方法是如何处理解决问题的。

可是如果想再进一步理解其中的原理的话，那背后就会涉及到一些数学上的推导，因此也需要掌握一些**<u>微积分、概率、线性代数</u>**等数学基础才能够理解。

我自己从大学到研究生一直学的是数学专业，一直以来自己也没有系统的梳理过数学的体系，直到工作了几年，才逐渐意识到数学的作用，而且很多人对数学是有种排斥和焦虑的，这其中也包括像我学习了七八年专业数学知识的人，也一样会有这种感觉，（划水去了）。但是我想表达的是有这种焦虑的感觉是正常的，数学这门学科是需要时间和精力去学习和投入的，而且也值得我们去学会运用数学，学会如何化繁为简，学会去解决工程问题，学会去跳出固有思维。

注：本系列博客是记录自己在梳理机器学习基础数学知识的过程当中的一些理解和笔记，本篇为系列基础第一篇。

---



### 从导数到梯度下降：

从高中阶段开始，我们就接触到了**导数**，它的标准叫法叫做导函数（derived function),它是一个函数而不是数字，（不要被名字所误导了）。对于一个函数
$$
y=f(x)         \tag{1}\\
$$
,在开区间内每一点都可导，就称函数f(x)在区间内可导,（这里的区间就是x定义的范围）（并不是所有的函数在其定义的范围内都存在导函数）它的导数可用
$$
f^{'}(x)，（也可以记做 \frac{df(x)}{dx}）
$$


来表示，在几何意义上就是函数曲线上的切线斜率。[维基百科-导数](https://en.wikipedia.org/wiki/Derivative)

我们知道很多时候，对于一个函数，我们无法求出它的解析解（比如对于一个二次函数，我们通过配方移向等方法就可以得到它的根，这里的根，我们是可以直接求解出来的），那这个时候我们可以考虑利用迭代的思想（逼近的方式来接近理论上的精确值）



---

谈到**迭代法**，就不得不提到Newton's method ,它是Newton 提出的一种近似求解方程的方法。

对于方程（1） 给定一个初始值
$$
x_{0},代入就可以表示平面直角坐标系上一点（x_{0}，f(x_{0}))
$$
这个点落在曲线上。假设我们的方程（1）为
$$
y=wx+b     \tag{2}\\
$$
我们可以获得切线方程为
$$
y=f(x_{0})+f^{'}(x_{0})(x-x_{0})
$$
这里的
$$
w=f^{'}(x) , b=f(x_{0})-f^{'}(x_{0})x_{0}
$$
w是斜率，b就是截距。令y=0,则我们可以获得
$$
x_{1}=x_{0}-\frac{f(x_{0})}{f^{’}(x_{0})}
$$
同理类推我们就可以得到
$$
x_{n}=x_{n-1}-\frac{f(x_{n-1})}{f^{’}(x_{n-1})}
$$
这就达到了逐渐逼近最优解的过程。利用这种思想我们引申到讨论最基础的线性模型问题。



---

谈到**线性模型**，我们只考虑监督学习下的多元线性模型，（这里的多元就是多个变量的意思）

我们用
$$
y来表示真实值，\underline y来表示模型计算的预测值
$$
我们用参数 e 代表 error ，表示真实值与预测值之间的接近程度。模型预测的值与训练集数据的差异称为残差（residuals）或训练误差（training errors）,用E代表残差。
$$
e_{i}=y_{i}-\underline{y_{i}}
$$
这里的下标 i 表示第 i 个样本，每一个的表达式都应该是这样 。 所有样本求和则变成了：
$$
E=\sum_{i=1}^n{e_{i}}=\sum_{i=1}^n{(y_{i}-\underline{y_{i}})}=\sum_{i=1}^n{(y_{i}-(wx_{i}+b))}
$$
但是我们知道
$$
每一个e_{i}
$$
本身是正是负都应该算作residuals，那么让其内部正负抵消显然不合适，这种情况下应该把它 都做一个非负的处理，令
$$
b=w_{0}
$$
变为
$$
\begin{align}
E &= \frac{1}{2}\sum_{i=1}^n{(y_{i}-(wx_{i}+b))}^{2}  \tag{3}\\
  &= \frac{1}{2}\sum_{i=1}^n{(y_{i}-W^{T}x_{i})}^{2}  \tag{4}\\

\end{align}
$$
其中（3）（4）前面的数字是为了方便后续求导方便，而设定的。

模型的训练，实际上就是要取到合适的W，使得（4）取得最小值，它就是我们的目标函数，而要求解目标函数，就是需要利用迭代的思想。我们可以把（4）看作是W的函数，利用梯度下降算法来优化这个目标函数。



---

谈到**梯度下降**，梯度是一个向量，它代表的是这个函数上升最快的方向，梯度就是沿着梯度负的方向去修改x的值，从而使函数取到函数最小值附近。这里就需要上文提到的迭代思想，我们就可以类似的得到关于W的梯度下降算法公式。[为什么梯度反方向是函数值下降最快的方向](https://www.zybuluo.com/irving512/note/929786)
$$
W_{n}=W_{o}-\eta\nabla E  \tag{5}\\
$$
其中
$$
\begin{align}
\nabla E &= \frac{\partial }{\partial W} E(W) \tag{6}\\
&= \frac{\partial }{\partial W}  \frac{1}{2}\sum_{i=1}^n{(y_{i}-W^{T}x_{i})}^{2} \tag{7}\\
&=\frac{1}{2}\sum_{i=1}^n \frac{\partial }{\partial W}  {(y_{i}-W^{T}x_{i})}^{2} \tag{8}\\
&=\frac{1}{2}\sum_{i=1}^n \frac{\partial }{\partial W}  {(y_{i}^{2}-2W^{T}x_{i}y_{i}+(W^{T}x_{i})^{2})} \tag{9}\\
&=\sum_{i=1}^n {(-y_{i}+(W^{T}x_{i}))x_{i}} \tag{10}\\
&=\sum_{i=1}^n {(-y_{i}+\underline{y_{i}})x_{i}} \tag{11}\\
\end{align}
$$


即
$$
\begin{align}
W_{n} &=W_{o}-\eta\nabla E  \tag{12}\\
&=W_{o}-\eta \sum_{i=1}^n {(-y_{i}+\underline{y_{i}})x_{i}} \tag{13}\\
\end{align}
$$
自此更新迭代W的算式已经求解，可以很容易根据（13）来实现相应的求解线性模型代码。

