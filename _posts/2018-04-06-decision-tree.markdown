---
layout: post
title: Decision-Tree
categories: [机器学习]
comments: true
description: 决策树相关基础要点
---

**决策树**

------

 决策树可以看成一个if-then规则集合，具有“互斥完备”性质 。决策树基本上都是采用的是贪心的算法，自顶向下递归分治构造。如基础的ID3,C4.5,CART等，各种决策树基础算法之间的主要区别在于分成的组之间差异的衡量方式不一样。ID3基于信息增益，C4.5基于信息增益率，CART基于基尼不纯度来划分。
。

**决策树是一种常用的机器学习算法**，它基于树状结构来进行决策。以下是决策树的基础要点：

1. 特征选择：决策树的第一步是选择合适的特征来构建树。特征选择的目标是找到能够最好地分割数据集的特征。

2. 决策节点：决策节点是决策树中的每个分支点，它根据某个特征的取值将数据集分割成更小的子集。

3. 叶节点：叶节点是决策树的最终输出。它代表了决策树对输入数据的预测结果或分类标签。

4. 分裂准则：决策树在每个决策节点上选择分裂准则来确定如何划分数据集。常见的分裂准则包括基尼系数和信息增益。

5. 剪枝：决策树容易过拟合训练数据，剪枝是一种常用的防止过拟合的方法。剪枝通过去掉一些决策节点或子树来简化模型。

**机器学习中常见的决策树算法应用包括：**

1. 分类问题：决策树可以用于解决分类问题，如垃圾邮件过滤、疾病诊断等。

2. 回归问题：决策树也可以用于回归问题，如预测房价、销售量等连续型变量。

3. 特征选择：决策树的特征选择方法可以用于其他机器学习算法中的特征选择步骤，帮助提升模型的性能和解释能力。

4. 集成学习：决策树可以作为集成学习中的基础模型，如随机森林、梯度提升树等。


------
GBDT（梯度提升决策树）
而Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度方向上建立一个新的模型。
GBDT算法框架中放入决策树，就是GBDT，分为两个版本：
**残差版本** 
**梯度版本** 
两者的不同主要在于每步迭代时，是否使用梯度作为求解方法。前者不用梯度而是用残差—-**残差是全局最优值**，梯度是局部最优方向步长，即前者每一步都在试图让结果变成最好，后者则每步试图让结果更好一点。

------

在Python中，有几个常用的库及其对应的决策树算法：
1. 如scikit-learn通过使用 DecisionTreeClassifier 和 DecisionTreeRegressor 类，可以实现分类和回归决策树。
2. XGBoost：XGBoost 是一种梯度提升框架，其中包含了决策树作为基础模型。它是一个快速、高效并且广泛使用的库，适用于分类和回归问题。
3. LightGBM：LightGBM 是另一个梯度提升框架，它也支持决策树作为基础模型。LightGBM 的设计目标是高效、快速的处理大规模数据集。
4. CatBoost 是一种基于梯度提升框架的机器学习算法，专门用于解决分类和回归问题。它具有高性能、高准确率和鲁棒性等特点，并且能够自动处理类别型特征和缺失值。
